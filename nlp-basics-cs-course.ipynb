{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brown.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=brown.sents(categories='news')[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\twink\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"it was a very pleasent day ,the weather was cool and there were light showers. I went to the market to buy some fruits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it was a very pleasent day ,the weather was cool and there were light showers.',\n",
       " 'I went to the market to buy some fruits']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(sentences[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'a',\n",
       " 'very',\n",
       " 'pleasent',\n",
       " 'day',\n",
       " ',',\n",
       " 'the',\n",
       " 'weather',\n",
       " 'was',\n",
       " 'cool',\n",
       " 'and',\n",
       " 'there',\n",
       " 'were',\n",
       " 'light',\n",
       " 'showers',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sws=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "print(len(sws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_words=[w for w in words if w not in sws]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pleasent', 'day', ',', 'weather', 'cool', 'light', 'showers', '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using a regular language tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_tokenize=RegexpTokenizer(\"[a-zA-Z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenized=reg_tokenize.tokenize(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'a',\n",
       " 'very',\n",
       " 'pleasent',\n",
       " 'day',\n",
       " 'the',\n",
       " 'weather',\n",
       " 'was',\n",
       " 'cool',\n",
       " 'and',\n",
       " 'there',\n",
       " 'were',\n",
       " 'light',\n",
       " 'showers']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_words_2=[w for w in text_tokenized if w not in sws]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pleasent', 'day', 'weather', 'cool', 'light', 'showers']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_words_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Where did he learn to dance like that?',\n",
    "        'His eyes were dancing with humor.',\n",
    "        'She shook her head and danced away',\n",
    "        'Alex was an excellent dancer. \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=reg_tokenize.tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where',\n",
       " 'did',\n",
       " 'he',\n",
       " 'learn',\n",
       " 'to',\n",
       " 'dance',\n",
       " 'like',\n",
       " 'that',\n",
       " 'his',\n",
       " 'eyes',\n",
       " 'were',\n",
       " 'dancing',\n",
       " 'with',\n",
       " 'humor',\n",
       " 'she',\n",
       " 'shook',\n",
       " 'her',\n",
       " 'head',\n",
       " 'and',\n",
       " 'danced',\n",
       " 'away',\n",
       " 'alex',\n",
       " 'was',\n",
       " 'an',\n",
       " 'excellent',\n",
       " 'dancer']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=[w for w in word_list if w not in sws]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learn',\n",
       " 'dance',\n",
       " 'like',\n",
       " 'eyes',\n",
       " 'dancing',\n",
       " 'humor',\n",
       " 'shook',\n",
       " 'head',\n",
       " 'danced',\n",
       " 'away',\n",
       " 'alex',\n",
       " 'excellent',\n",
       " 'dancer']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list# only usedul words but has other root words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "1)snow ball stemmer\n",
    "2) porter\n",
    "3) lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    " ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"jumping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"jumps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awesom'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"awesome\")# some dont make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_by_ps=[ps.stem(w) for w in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learn',\n",
       " 'danc',\n",
       " 'like',\n",
       " 'eye',\n",
       " 'danc',\n",
       " 'humor',\n",
       " 'shook',\n",
       " 'head',\n",
       " 'danc',\n",
       " 'away',\n",
       " 'alex',\n",
       " 'excel',\n",
       " 'dancer']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer_by_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learn',\n",
       " 'dant',\n",
       " 'lik',\n",
       " 'ey',\n",
       " 'dant',\n",
       " 'hum',\n",
       " 'shook',\n",
       " 'head',\n",
       " 'dant',\n",
       " 'away',\n",
       " 'alex',\n",
       " 'excel',\n",
       " 'dant']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer_by_ls=[ls.stem(w) for w in word_list]\n",
    "stemmer_by_ls# different results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learn', 'danc', 'like', 'eye', 'danc', 'humor', 'shook', 'head', 'danc', 'away', 'alex', 'excel', 'dancer']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "ss=SnowballStemmer(language='english')\n",
    "stemmer_by_ss=[ss.stem(w.strip()) for w in word_list]\n",
    "print(stemmer_by_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building common vocalbulary and vectoring documents(Bag of EWords approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text=[\"We cannot work with text directly when using machine learning algorithms.\",\n",
    "\n",
    "\"Instead, we need to convert the text to numbers\"\n",
    ",\n",
    "\"We may want to perform classification of documents, so each document is an “input” and a class label is the “output” for our predictive algorithm. Algorithms take vectors of numbers as input, therefore we need to convert documents to fixed-length vectors of numbers.\"\n",
    ",\n",
    "\"A simple and effective model for thinking about text documents in machine learning is called the Bag-of-Words Model, or BoW.\"\n",
    ",\n",
    "\"The model is simple in that it throws away all of the order information in the words and focuses on the occurrence of words in a document.\"\n",
    ",\"This can be done by assigning each word a unique number. Then any document we see can be encoded as a fixed-length vector with the length of the vocabulary of known words. The value in each position in the vector could be filled with a count or frequency of each word in the encoded document  \"\n",
    "\n",
    ",\"This is the bag of words model, where we are only concerned with encoding schemes that represent what words are present or the degree to which they are present in encoded documents without any information about order. \"\n",
    ",\"There are many ways to extend this simple method, both by better clarifying what a “word” is and in defining what to encode about each word in the vector\"\n",
    "\n",
    ",\"The scikit-learn library provides 3 different schemes that we can use, and we will briefly look at each\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We cannot work with text directly when using machine learning algorithms.',\n",
       " 'Instead, we need to convert the text to numbers',\n",
       " 'We may want to perform classification of documents, so each document is an “input” and a class label is the “output” for our predictive algorithm. Algorithms take vectors of numbers as input, therefore we need to convert documents to fixed-length vectors of numbers.',\n",
       " 'A simple and effective model for thinking about text documents in machine learning is called the Bag-of-Words Model, or BoW.',\n",
       " 'The model is simple in that it throws away all of the order information in the words and focuses on the occurrence of words in a document.',\n",
       " 'This can be done by assigning each word a unique number. Then any document we see can be encoded as a fixed-length vector with the length of the vocabulary of known words. The value in each position in the vector could be filled with a count or frequency of each word in the encoded document  ',\n",
       " 'This is the bag of words model, where we are only concerned with encoding schemes that represent what words are present or the degree to which they are present in encoded documents without any information about order. ',\n",
       " 'There are many ways to extend this simple method, both by better clarifying what a “word” is and in defining what to encode about each word in the vector',\n",
       " 'The scikit-learn library provides 3 different schemes that we can use, and we will briefly look at each']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=CountVectorizer()\n",
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorised_corpus=cv.fit_transform(new_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 2 0]\n",
      " [1 0 0 ... 2 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorised_corpus)# each row each word frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'algorithm', 'algorithms', 'all', 'an', 'and', 'any', 'are', 'as', 'assigning', 'at', 'away', 'bag', 'be', 'better', 'both', 'bow', 'briefly', 'by', 'called', 'can', 'cannot', 'clarifying', 'class', 'classification', 'concerned', 'convert', 'could', 'count', 'defining', 'degree', 'different', 'directly', 'document', 'documents', 'done', 'each', 'effective', 'encode', 'encoded', 'encoding', 'extend', 'filled', 'fixed', 'focuses', 'for', 'frequency', 'in', 'information', 'input', 'instead', 'is', 'it', 'known', 'label', 'learn', 'learning', 'length', 'library', 'look', 'machine', 'many', 'may', 'method', 'model', 'need', 'number', 'numbers', 'occurrence', 'of', 'on', 'only', 'or', 'order', 'our', 'output', 'perform', 'position', 'predictive', 'present', 'provides', 'represent', 'schemes', 'scikit', 'see', 'simple', 'so', 'take', 'text', 'that', 'the', 'then', 'there', 'therefore', 'they', 'thinking', 'this', 'throws', 'to', 'unique', 'use', 'using', 'value', 'vector', 'vectors', 'vocabulary', 'want', 'ways', 'we', 'what', 'when', 'where', 'which', 'will', 'with', 'without', 'word', 'words', 'work']\n"
     ]
    }
   ],
   "source": [
    "print((sorted(cv.vocabulary_)))# word-> index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 119)\n"
     ]
    }
   ],
   "source": [
    "print(vectorised_corpus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_dimensional=vectorised_corpus.reshape((9*119,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_dimensional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['algorithms', 'cannot', 'directly', 'learning', 'machine', 'text',\n",
      "       'using', 'we', 'when', 'with', 'work'], dtype='<U14'), array(['convert', 'instead', 'need', 'numbers', 'text', 'the', 'to', 'we'],\n",
      "      dtype='<U14'), array(['algorithm', 'algorithms', 'an', 'and', 'as', 'class',\n",
      "       'classification', 'convert', 'document', 'documents', 'each',\n",
      "       'fixed', 'for', 'input', 'is', 'label', 'length', 'may', 'need',\n",
      "       'numbers', 'of', 'our', 'output', 'perform', 'predictive', 'so',\n",
      "       'take', 'the', 'therefore', 'to', 'vectors', 'want', 'we'],\n",
      "      dtype='<U14'), array(['about', 'and', 'bag', 'bow', 'called', 'documents', 'effective',\n",
      "       'for', 'in', 'is', 'learning', 'machine', 'model', 'of', 'or',\n",
      "       'simple', 'text', 'the', 'thinking', 'words'], dtype='<U14'), array(['all', 'and', 'away', 'document', 'focuses', 'in', 'information',\n",
      "       'is', 'it', 'model', 'occurrence', 'of', 'on', 'order', 'simple',\n",
      "       'that', 'the', 'throws', 'words'], dtype='<U14'), array(['any', 'as', 'assigning', 'be', 'by', 'can', 'could', 'count',\n",
      "       'document', 'done', 'each', 'encoded', 'filled', 'fixed',\n",
      "       'frequency', 'in', 'known', 'length', 'number', 'of', 'or',\n",
      "       'position', 'see', 'the', 'then', 'this', 'unique', 'value',\n",
      "       'vector', 'vocabulary', 'we', 'with', 'word', 'words'],\n",
      "      dtype='<U14'), array(['about', 'any', 'are', 'bag', 'concerned', 'degree', 'documents',\n",
      "       'encoded', 'encoding', 'in', 'information', 'is', 'model', 'of',\n",
      "       'only', 'or', 'order', 'present', 'represent', 'schemes', 'that',\n",
      "       'the', 'they', 'this', 'to', 'we', 'what', 'where', 'which',\n",
      "       'with', 'without', 'words'], dtype='<U14'), array(['about', 'and', 'are', 'better', 'both', 'by', 'clarifying',\n",
      "       'defining', 'each', 'encode', 'extend', 'in', 'is', 'many',\n",
      "       'method', 'simple', 'the', 'there', 'this', 'to', 'vector', 'ways',\n",
      "       'what', 'word'], dtype='<U14'), array(['and', 'at', 'briefly', 'can', 'different', 'each', 'learn',\n",
      "       'library', 'look', 'provides', 'schemes', 'scikit', 'that', 'the',\n",
      "       'use', 'we', 'will'], dtype='<U14')]\n"
     ]
    }
   ],
   "source": [
    "print(cv.inverse_transform(vectorised_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector=np.ones((37,))\n",
    "vector[4:9]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['about', 'algorithm', 'algorithms', 'all', 'assigning', 'at',\n",
      "       'away', 'bag', 'be', 'better', 'both', 'bow', 'briefly', 'by',\n",
      "       'called', 'can', 'cannot', 'clarifying', 'class', 'classification',\n",
      "       'concerned', 'convert', 'could', 'count', 'defining', 'degree',\n",
      "       'different', 'directly', 'document', 'documents', 'done', 'each'],\n",
      "      dtype='<U14')]\n"
     ]
    }
   ],
   "source": [
    "print(cv.inverse_transform(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_[\"we\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTokenizer(sentence):\n",
    "    words=reg_tokenize.tokenize(sentence)\n",
    "    words=[w for w in words if w not in sws]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cannot', 'work', 'text', 'directly', 'using', 'machine', 'learning', 'algorithms']\n",
      "['instead', 'need', 'convert', 'text', 'numbers']\n",
      "['may', 'want', 'perform', 'classification', 'documents', 'document', 'input', 'class', 'label', 'output', 'predictive', 'algorithm', 'algorithms', 'take', 'vectors', 'numbers', 'input', 'therefore', 'need', 'convert', 'documents', 'fixed', 'length', 'vectors', 'numbers']\n",
      "['simple', 'effective', 'model', 'thinking', 'text', 'documents', 'machine', 'learning', 'called', 'bag', 'words', 'model', 'bow']\n",
      "['model', 'simple', 'throws', 'away', 'order', 'information', 'words', 'focuses', 'occurrence', 'words', 'document']\n",
      "['done', 'assigning', 'word', 'unique', 'number', 'document', 'see', 'encoded', 'fixed', 'length', 'vector', 'length', 'vocabulary', 'known', 'words', 'value', 'position', 'vector', 'could', 'filled', 'count', 'frequency', 'word', 'encoded', 'document']\n",
      "['bag', 'words', 'model', 'concerned', 'encoding', 'schemes', 'represent', 'words', 'present', 'degree', 'present', 'encoded', 'documents', 'without', 'information', 'order']\n",
      "['many', 'ways', 'extend', 'simple', 'method', 'better', 'clarifying', 'word', 'defining', 'encode', 'word', 'vector']\n",
      "['scikit', 'learn', 'library', 'provides', 'different', 'schemes', 'use', 'briefly', 'look']\n"
     ]
    }
   ],
   "source": [
    "for nt in new_text:\n",
    "    print(myTokenizer(nt.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t1\n",
      "  (0, 81)\t1\n",
      "  (0, 65)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 71)\t1\n",
      "  (0, 43)\t1\n",
      "  (0, 39)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 65)\t1\n",
      "  (1, 35)\t1\n",
      "  (1, 48)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 50)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 48)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 50)\t2\n",
      "  (2, 45)\t1\n",
      "  (2, 76)\t1\n",
      "  (2, 54)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 22)\t2\n",
      "  (2, 21)\t1\n",
      "  (2, 34)\t2\n",
      "  (2, 11)\t1\n",
      "  :\t:\n",
      "  (6, 60)\t1\n",
      "  (6, 59)\t1\n",
      "  (6, 57)\t2\n",
      "  (6, 18)\t1\n",
      "  (6, 78)\t1\n",
      "  (7, 63)\t1\n",
      "  (7, 79)\t2\n",
      "  (7, 73)\t1\n",
      "  (7, 44)\t1\n",
      "  (7, 77)\t1\n",
      "  (7, 28)\t1\n",
      "  (7, 46)\t1\n",
      "  (7, 5)\t1\n",
      "  (7, 10)\t1\n",
      "  (7, 17)\t1\n",
      "  (7, 25)\t1\n",
      "  (8, 60)\t1\n",
      "  (8, 61)\t1\n",
      "  (8, 38)\t1\n",
      "  (8, 41)\t1\n",
      "  (8, 58)\t1\n",
      "  (8, 19)\t1\n",
      "  (8, 70)\t1\n",
      "  (8, 7)\t1\n",
      "  (8, 42)\t1\n"
     ]
    }
   ],
   "source": [
    "cv2=CountVectorizer(tokenizer=myTokenizer)\n",
    "#for nn in new_text:\n",
    "vectorised_corpus_new=cv2.fit_transform(new_text)\n",
    "print(vectorised_corpus_new) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 1]]\n",
      "[array(['algorithms', 'cannot', 'directly', 'learning', 'machine', 'text',\n",
      "       'using', 'work'], dtype='<U14')]\n"
     ]
    }
   ],
   "source": [
    "vcv=vectorised_corpus_new[0]\n",
    "print(vcv.toarray())\n",
    "print(cv2.inverse_transform(vcv.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bi gram , tri gram , n grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cannot': 24, 'work': 293, 'text': 227, 'directly': 59, 'using': 248, 'machine': 146, 'learning': 131, 'algorithms': 3, 'cannot work': 25, 'work text': 294, 'text directly': 228, 'directly using': 60, 'using machine': 249, 'machine learning': 147, 'learning algorithms': 132, 'cannot work text': 26, 'work text directly': 295, 'text directly using': 229, 'directly using machine': 61, 'using machine learning': 250, 'machine learning algorithms': 148, 'instead': 119, 'need': 167, 'convert': 39, 'numbers': 174, 'instead need': 120, 'need convert': 168, 'convert text': 42, 'text numbers': 232, 'instead need convert': 121, 'need convert text': 170, 'convert text numbers': 43, 'may': 153, 'want': 265, 'perform': 186, 'classification': 33, 'documents': 67, 'document': 62, 'input': 114, 'class': 30, 'label': 125, 'output': 183, 'predictive': 192, 'algorithm': 0, 'take': 224, 'vectors': 259, 'therefore': 233, 'fixed': 100, 'length': 135, 'may want': 154, 'want perform': 266, 'perform classification': 187, 'classification documents': 34, 'documents document': 68, 'document input': 63, 'input class': 115, 'class label': 31, 'label output': 126, 'output predictive': 184, 'predictive algorithm': 193, 'algorithm algorithms': 1, 'algorithms take': 4, 'take vectors': 225, 'vectors numbers': 260, 'numbers input': 175, 'input therefore': 117, 'therefore need': 234, 'convert documents': 40, 'documents fixed': 70, 'fixed length': 101, 'length vectors': 138, 'may want perform': 155, 'want perform classification': 267, 'perform classification documents': 188, 'classification documents document': 35, 'documents document input': 69, 'document input class': 64, 'input class label': 116, 'class label output': 32, 'label output predictive': 127, 'output predictive algorithm': 185, 'predictive algorithm algorithms': 194, 'algorithm algorithms take': 2, 'algorithms take vectors': 5, 'take vectors numbers': 226, 'vectors numbers input': 261, 'numbers input therefore': 176, 'input therefore need': 118, 'therefore need convert': 235, 'need convert documents': 169, 'convert documents fixed': 41, 'documents fixed length': 71, 'fixed length vectors': 103, 'length vectors numbers': 139, 'simple': 217, 'effective': 79, 'model': 159, 'thinking': 236, 'called': 21, 'bag': 12, 'words': 282, 'bow': 18, 'simple effective': 218, 'effective model': 80, 'model thinking': 165, 'thinking text': 237, 'text documents': 230, 'documents machine': 72, 'learning called': 133, 'called bag': 22, 'bag words': 13, 'words model': 286, 'model bow': 160, 'simple effective model': 219, 'effective model thinking': 81, 'model thinking text': 166, 'thinking text documents': 238, 'text documents machine': 231, 'documents machine learning': 73, 'machine learning called': 149, 'learning called bag': 134, 'called bag words': 23, 'bag words model': 14, 'words model bow': 287, 'throws': 239, 'away': 9, 'order': 180, 'information': 110, 'focuses': 104, 'occurrence': 177, 'model simple': 163, 'simple throws': 222, 'throws away': 240, 'away order': 10, 'order information': 181, 'information words': 112, 'words focuses': 284, 'focuses occurrence': 105, 'occurrence words': 178, 'words document': 283, 'model simple throws': 164, 'simple throws away': 223, 'throws away order': 241, 'away order information': 11, 'order information words': 182, 'information words focuses': 113, 'words focuses occurrence': 285, 'focuses occurrence words': 106, 'occurrence words document': 179, 'done': 76, 'assigning': 6, 'word': 274, 'unique': 242, 'number': 171, 'see': 214, 'encoded': 85, 'vector': 254, 'vocabulary': 262, 'known': 122, 'value': 251, 'position': 189, 'could': 44, 'filled': 97, 'count': 47, 'frequency': 107, 'done assigning': 77, 'assigning word': 7, 'word unique': 279, 'unique number': 243, 'number document': 172, 'document see': 65, 'see encoded': 215, 'encoded fixed': 89, 'length vector': 136, 'vector length': 257, 'length vocabulary': 140, 'vocabulary known': 263, 'known words': 123, 'words value': 291, 'value position': 252, 'position vector': 190, 'vector could': 255, 'could filled': 45, 'filled count': 98, 'count frequency': 48, 'frequency word': 108, 'word encoded': 277, 'encoded document': 86, 'done assigning word': 78, 'assigning word unique': 8, 'word unique number': 280, 'unique number document': 244, 'number document see': 173, 'document see encoded': 66, 'see encoded fixed': 216, 'encoded fixed length': 90, 'fixed length vector': 102, 'length vector length': 137, 'vector length vocabulary': 258, 'length vocabulary known': 141, 'vocabulary known words': 264, 'known words value': 124, 'words value position': 292, 'value position vector': 253, 'position vector could': 191, 'vector could filled': 256, 'could filled count': 46, 'filled count frequency': 99, 'count frequency word': 49, 'frequency word encoded': 109, 'word encoded document': 278, 'concerned': 36, 'encoding': 91, 'schemes': 206, 'represent': 203, 'present': 195, 'degree': 53, 'without': 271, 'model concerned': 161, 'concerned encoding': 37, 'encoding schemes': 92, 'schemes represent': 207, 'represent words': 204, 'words present': 289, 'present degree': 196, 'degree present': 54, 'present encoded': 198, 'encoded documents': 87, 'documents without': 74, 'without information': 272, 'information order': 111, 'words model concerned': 288, 'model concerned encoding': 162, 'concerned encoding schemes': 38, 'encoding schemes represent': 93, 'schemes represent words': 208, 'represent words present': 205, 'words present degree': 290, 'present degree present': 197, 'degree present encoded': 55, 'present encoded documents': 199, 'encoded documents without': 88, 'documents without information': 75, 'without information order': 273, 'many': 150, 'ways': 268, 'extend': 94, 'method': 156, 'better': 15, 'clarifying': 27, 'defining': 50, 'encode': 82, 'many ways': 151, 'ways extend': 269, 'extend simple': 95, 'simple method': 220, 'method better': 157, 'better clarifying': 16, 'clarifying word': 28, 'word defining': 275, 'defining encode': 51, 'encode word': 83, 'word vector': 281, 'many ways extend': 152, 'ways extend simple': 270, 'extend simple method': 96, 'simple method better': 221, 'method better clarifying': 158, 'better clarifying word': 17, 'clarifying word defining': 29, 'word defining encode': 276, 'defining encode word': 52, 'encode word vector': 84, 'scikit': 211, 'learn': 128, 'library': 142, 'provides': 200, 'different': 56, 'use': 245, 'briefly': 19, 'look': 145, 'scikit learn': 212, 'learn library': 129, 'library provides': 143, 'provides different': 201, 'different schemes': 57, 'schemes use': 209, 'use briefly': 246, 'briefly look': 20, 'scikit learn library': 213, 'learn library provides': 130, 'library provides different': 144, 'provides different schemes': 202, 'different schemes use': 58, 'schemes use briefly': 210, 'use briefly look': 247}\n"
     ]
    }
   ],
   "source": [
    "cv3=CountVectorizer(tokenizer=myTokenizer,ngram_range=(1,3))# work with unigram,bigram and trigrams\n",
    "vectorised_corpus_new=cv3.fit_transform(new_text)\n",
    "print(cv3.vocabulary_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296\n"
     ]
    }
   ],
   "source": [
    "print(len(cv3.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 62)\t1\n",
      "  (0, 217)\t1\n",
      "  (0, 159)\t1\n",
      "  (0, 282)\t2\n",
      "  (0, 239)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 180)\t1\n",
      "  (0, 110)\t1\n",
      "  (0, 104)\t1\n",
      "  (0, 177)\t1\n",
      "  (0, 163)\t1\n",
      "  (0, 222)\t1\n",
      "  (0, 240)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 181)\t1\n",
      "  (0, 112)\t1\n",
      "  (0, 284)\t1\n",
      "  (0, 105)\t1\n",
      "  (0, 178)\t1\n",
      "  (0, 283)\t1\n",
      "  (0, 164)\t1\n",
      "  (0, 223)\t1\n",
      "  (0, 241)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 182)\t1\n",
      "  :\t:\n",
      "  (3, 84)\t1\n",
      "  (4, 206)\t1\n",
      "  (4, 211)\t1\n",
      "  (4, 128)\t1\n",
      "  (4, 142)\t1\n",
      "  (4, 200)\t1\n",
      "  (4, 56)\t1\n",
      "  (4, 245)\t1\n",
      "  (4, 19)\t1\n",
      "  (4, 145)\t1\n",
      "  (4, 212)\t1\n",
      "  (4, 129)\t1\n",
      "  (4, 143)\t1\n",
      "  (4, 201)\t1\n",
      "  (4, 57)\t1\n",
      "  (4, 209)\t1\n",
      "  (4, 246)\t1\n",
      "  (4, 20)\t1\n",
      "  (4, 213)\t1\n",
      "  (4, 130)\t1\n",
      "  (4, 144)\t1\n",
      "  (4, 202)\t1\n",
      "  (4, 58)\t1\n",
      "  (4, 210)\t1\n",
      "  (4, 247)\t1\n"
     ]
    }
   ],
   "source": [
    "print(vectorised_corpus_new[4:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nf-idf normalizaionm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight=tf* idf\n",
    "# tf(t,d)-> how many times t occurs in docment d\n",
    "# idf(t,,c ) -> log(1/(1+count(d,t))) => all occurences of t in the corpus c across all the documents in c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vctorizer=TfidfVectorizer(tokenizer=myTokenizer,ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.22708043 0.22708043 0.22708043]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.1142998  0.1142998  0.1142998  ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "{'cannot': 24, 'work': 293, 'text': 227, 'directly': 59, 'using': 248, 'machine': 146, 'learning': 131, 'algorithms': 3, 'cannot work': 25, 'work text': 294, 'text directly': 228, 'directly using': 60, 'using machine': 249, 'machine learning': 147, 'learning algorithms': 132, 'cannot work text': 26, 'work text directly': 295, 'text directly using': 229, 'directly using machine': 61, 'using machine learning': 250, 'machine learning algorithms': 148, 'instead': 119, 'need': 167, 'convert': 39, 'numbers': 174, 'instead need': 120, 'need convert': 168, 'convert text': 42, 'text numbers': 232, 'instead need convert': 121, 'need convert text': 170, 'convert text numbers': 43, 'may': 153, 'want': 265, 'perform': 186, 'classification': 33, 'documents': 67, 'document': 62, 'input': 114, 'class': 30, 'label': 125, 'output': 183, 'predictive': 192, 'algorithm': 0, 'take': 224, 'vectors': 259, 'therefore': 233, 'fixed': 100, 'length': 135, 'may want': 154, 'want perform': 266, 'perform classification': 187, 'classification documents': 34, 'documents document': 68, 'document input': 63, 'input class': 115, 'class label': 31, 'label output': 126, 'output predictive': 184, 'predictive algorithm': 193, 'algorithm algorithms': 1, 'algorithms take': 4, 'take vectors': 225, 'vectors numbers': 260, 'numbers input': 175, 'input therefore': 117, 'therefore need': 234, 'convert documents': 40, 'documents fixed': 70, 'fixed length': 101, 'length vectors': 138, 'may want perform': 155, 'want perform classification': 267, 'perform classification documents': 188, 'classification documents document': 35, 'documents document input': 69, 'document input class': 64, 'input class label': 116, 'class label output': 32, 'label output predictive': 127, 'output predictive algorithm': 185, 'predictive algorithm algorithms': 194, 'algorithm algorithms take': 2, 'algorithms take vectors': 5, 'take vectors numbers': 226, 'vectors numbers input': 261, 'numbers input therefore': 176, 'input therefore need': 118, 'therefore need convert': 235, 'need convert documents': 169, 'convert documents fixed': 41, 'documents fixed length': 71, 'fixed length vectors': 103, 'length vectors numbers': 139, 'simple': 217, 'effective': 79, 'model': 159, 'thinking': 236, 'called': 21, 'bag': 12, 'words': 282, 'bow': 18, 'simple effective': 218, 'effective model': 80, 'model thinking': 165, 'thinking text': 237, 'text documents': 230, 'documents machine': 72, 'learning called': 133, 'called bag': 22, 'bag words': 13, 'words model': 286, 'model bow': 160, 'simple effective model': 219, 'effective model thinking': 81, 'model thinking text': 166, 'thinking text documents': 238, 'text documents machine': 231, 'documents machine learning': 73, 'machine learning called': 149, 'learning called bag': 134, 'called bag words': 23, 'bag words model': 14, 'words model bow': 287, 'throws': 239, 'away': 9, 'order': 180, 'information': 110, 'focuses': 104, 'occurrence': 177, 'model simple': 163, 'simple throws': 222, 'throws away': 240, 'away order': 10, 'order information': 181, 'information words': 112, 'words focuses': 284, 'focuses occurrence': 105, 'occurrence words': 178, 'words document': 283, 'model simple throws': 164, 'simple throws away': 223, 'throws away order': 241, 'away order information': 11, 'order information words': 182, 'information words focuses': 113, 'words focuses occurrence': 285, 'focuses occurrence words': 106, 'occurrence words document': 179, 'done': 76, 'assigning': 6, 'word': 274, 'unique': 242, 'number': 171, 'see': 214, 'encoded': 85, 'vector': 254, 'vocabulary': 262, 'known': 122, 'value': 251, 'position': 189, 'could': 44, 'filled': 97, 'count': 47, 'frequency': 107, 'done assigning': 77, 'assigning word': 7, 'word unique': 279, 'unique number': 243, 'number document': 172, 'document see': 65, 'see encoded': 215, 'encoded fixed': 89, 'length vector': 136, 'vector length': 257, 'length vocabulary': 140, 'vocabulary known': 263, 'known words': 123, 'words value': 291, 'value position': 252, 'position vector': 190, 'vector could': 255, 'could filled': 45, 'filled count': 98, 'count frequency': 48, 'frequency word': 108, 'word encoded': 277, 'encoded document': 86, 'done assigning word': 78, 'assigning word unique': 8, 'word unique number': 280, 'unique number document': 244, 'number document see': 173, 'document see encoded': 66, 'see encoded fixed': 216, 'encoded fixed length': 90, 'fixed length vector': 102, 'length vector length': 137, 'vector length vocabulary': 258, 'length vocabulary known': 141, 'vocabulary known words': 264, 'known words value': 124, 'words value position': 292, 'value position vector': 253, 'position vector could': 191, 'vector could filled': 256, 'could filled count': 46, 'filled count frequency': 99, 'count frequency word': 49, 'frequency word encoded': 109, 'word encoded document': 278, 'concerned': 36, 'encoding': 91, 'schemes': 206, 'represent': 203, 'present': 195, 'degree': 53, 'without': 271, 'model concerned': 161, 'concerned encoding': 37, 'encoding schemes': 92, 'schemes represent': 207, 'represent words': 204, 'words present': 289, 'present degree': 196, 'degree present': 54, 'present encoded': 198, 'encoded documents': 87, 'documents without': 74, 'without information': 272, 'information order': 111, 'words model concerned': 288, 'model concerned encoding': 162, 'concerned encoding schemes': 38, 'encoding schemes represent': 93, 'schemes represent words': 208, 'represent words present': 205, 'words present degree': 290, 'present degree present': 197, 'degree present encoded': 55, 'present encoded documents': 199, 'encoded documents without': 88, 'documents without information': 75, 'without information order': 273, 'many': 150, 'ways': 268, 'extend': 94, 'method': 156, 'better': 15, 'clarifying': 27, 'defining': 50, 'encode': 82, 'many ways': 151, 'ways extend': 269, 'extend simple': 95, 'simple method': 220, 'method better': 157, 'better clarifying': 16, 'clarifying word': 28, 'word defining': 275, 'defining encode': 51, 'encode word': 83, 'word vector': 281, 'many ways extend': 152, 'ways extend simple': 270, 'extend simple method': 96, 'simple method better': 221, 'method better clarifying': 158, 'better clarifying word': 17, 'clarifying word defining': 29, 'word defining encode': 276, 'defining encode word': 52, 'encode word vector': 84, 'scikit': 211, 'learn': 128, 'library': 142, 'provides': 200, 'different': 56, 'use': 245, 'briefly': 19, 'look': 145, 'scikit learn': 212, 'learn library': 129, 'library provides': 143, 'provides different': 201, 'different schemes': 57, 'schemes use': 209, 'use briefly': 246, 'briefly look': 20, 'scikit learn library': 213, 'learn library provides': 130, 'library provides different': 144, 'provides different schemes': 202, 'different schemes use': 58, 'schemes use briefly': 210, 'use briefly look': 247}\n"
     ]
    }
   ],
   "source": [
    "vectorized_corpus=tfidf_vctorizer.fit_transform(new_text)\n",
    "print(vectorized_corpus.toarray())\n",
    "print(tfidf_vctorizer.vocabulary_)# each term has some weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
